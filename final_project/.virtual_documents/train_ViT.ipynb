











##define libraries
import os, gc
import numpy as np
import pandas as pd
from tqdm import tqdm
import torch
from torchvision import tv_tensors
import torchvision.transforms.functional as fn
from torchvision.io import read_image
from torch.utils.data import Dataset
from torchvision import datasets
from torch.utils.data import DataLoader
from torchvision.transforms import ToTensor
import matplotlib.pyplot as plt


##define paths
basepath = "C:/Users/vanth/OneDrive/Desktop/JHUClasses/data/numbers_mnist/"
trainpath = basepath + 'train/'
valpath = basepath + 'val/'


class CustomImageDataset(Dataset):
    def __init__(self, labelspath, imgspath, transform=None, target_transform=None):
        ##use dictionary to keep track of images
        data_dict = {}
        
        ##get the image file full paths
        imgfiles = sorted(os.listdir(imgspath))
        data_dict['imgpath'] = [imgspath + file for file in imgfiles]

        ##get the label file full paths
        labelfiles = sorted(os.listdir(labelspath))
        data_dict['labelpath'] = [labelspath + file for file in labelfiles]

        ##debug
        # print(data_dict['labelpath'])
        
        ##get labels and bboxes
        labels, bboxes = [], []
        for labelpath in tqdm(data_dict['labelpath'], 'Reading Labels'):
            with open(labelpath, 'r') as f:
                lines = f.readlines()
            
            ##a list of labels and bbox coordinates
            labels.append([line[0] for line in lines])
            bboxes.append([line[2:] for line in lines])

        ##remove strings from bbox coordinates
        for i, boxes in enumerate(bboxes):
            ##convert bbox strings to float coordinates
            boxes = [np.array(i.replace('\n', '').split(' ')).astype(float) for i in boxes]
            bboxes[i] = boxes
        print(boxes)
        
        data_dict['labels'] = labels
        data_dict['bboxes'] = bboxes
                
        ##convert dict to df
        self.df = pd.DataFrame(data_dict)
        
        ##define transforms
        self.transform = transform
        self.target_transform = target_transform

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        ##open image
        img_path = self.df.iloc[idx]['imgpath']
        image = fn.resize(read_image(img_path), size=[256, 256])
        print(image.shape)

        ##retrieve labels
        labels = torch.tensor(np.array(self.df.iloc[idx]['labels']).astype(int))
        print("Len Labels: ", labels.shape)
        
        ##retrieve bboxes
        bboxes = torch.tensor(np.array(self.df.iloc[idx]['bboxes']).astype(np.float64))
        print("Len Bboxes: ", bboxes.shape)
        
        ##transform image if applicable
        if self.transform:
            image = self.transform(image)
        if self.target_transform:
            label = self.target_transform(labels)
        return image, labels
        








from torchvision.transforms import v2
transforms = v2.Compose([
    v2.Normalize(mean=[0.485, 0.456], std=[0.229, 0.224]),])


%%time

##initialize datasets
train_data = CustomImageDataset(trainpath + 'labels/', trainpath + 'images/')
val_data = CustomImageDataset(valpath + 'labels/', valpath + 'images/')

gc.collect()


a = np.array(['1', '2']).astype(int)
torch.tensor(a)








%%time 

##define dataloaders
train_loader = DataLoader(train_data, batch_size=8, shuffle=True)
val_loader = DataLoader(val_data, batch_size=8, shuffle=True)

gc.collect()


%%time

##sanity check
# Display image and label.
train_features, train_labels = next(iter(train_loader))
print(f"Feature batch shape: {train_features.size()}")
print(f"Labels batch shape: {train_labels.size()}")
# print(f"Labels batch shape: {train_bboxes.size()}")
img = train_features[0].squeeze()
label = train_labels[0]
plt.imshow(img, cmap="gray")
plt.show()
print(f"Label: {label}")

gc.collect()



